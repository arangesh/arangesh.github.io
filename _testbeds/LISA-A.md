---
title: "LISA-A"
permalink: /testbeds/LISA-A
excerpt: ''
date: 2015-01-01
---
<p align="center">
 Â <img src="https://arangesh.github.io/images/LISA-A-im1.jpg?raw=true" alt="Photo" style="width: 350px;"/> 
</p>

Recent progress in autonomous and semiautonomous
driving has been made possible in part through
an assortment of sensors that provide the intelligent agent with
an enhanced perception of its surroundings. It has been clear
for quite some while now that for intelligent vehicles to function
effectively in all situations and conditions, a fusion of different
sensor technologies is essential. Consequently, the availability
of synchronized multi-sensory data streams are necessary to
promote the development of fusion based algorithms for low,
mid and high level semantic tasks. In this paper, we provide a
comprehensive description of our heavily sensorized, panoramic
testbed capable of providing high quality data from a slew of
synchronized and calibrated sensors such as cameras, LIDARs,
radars, and the IMU/GPS. The vehicle has recorded over 100
hours of real world data for a very diverse set of weather,
traffic and daylight conditions. All captured data is accurately
calibrated and synchronized using timestamps, and stored
safely in high performance servers mounted inside the vehicle
itself. Details on the testbed instrumentation, sensor layout,
sensor outputs, calibration and synchronization are described
in this paper.

[PDF link](http://cvrr.ucsd.edu/testbeds/lisa-a/info.pdf)

[Videos](https://www.youtube.com/watch?v=NN0rvKv-Aq8&feature=youtu.be)
