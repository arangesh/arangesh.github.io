---
title: "LISA-T"
permalink: /testbeds/LISA-T
excerpt: 'Tesla Model S platform'
---
<p align="center">
  <img src="https://arangesh.github.io/images/LISA-T-im1.jpg?raw=true" alt="Photo" style="width: 600px;"/> 
</p>

With increasing automated driving capabilities
of commercial vehicles, the study of safe and smooth
occupant-vehicle interaction and control transitions is key.
In this study, we focus on the development of contextual,
semantically meaningful representations of driver
and vehicle states, which can then be used to determine
the appropriate timing and conditions for transfer of
control between driver and vehicle. To this end, we lay
out the specifications of the vehicle platform required to
conduct such a study, and explore some of the sensors
and algorithms that may be needed to produce useful
and observable high level cues (features) to make such
decisions. These features encode different aspects of the
driver state, pertaining to the face, hands, foot and upper
body of the driver. Finally, we evaluate these features on
their capability of capturing the state of a driver, and
demonstrate a strong agreement between these features
and a humans’ notion of situational awareness.


[PDF link](file://132.239.223.20/web/publications/2018/LISAT.pdf)
